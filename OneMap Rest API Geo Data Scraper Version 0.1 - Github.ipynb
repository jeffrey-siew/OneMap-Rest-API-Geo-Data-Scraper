{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneMap.gov.sg Rest API Geo Data Scraper\n",
    "Introduction\n",
    "- This is a multiprocessing threadpool approach to scrap geo data (block number, road name, building name, address, postal code, latitude, longitude) from OneMap.gov.sg via using user definite search value.\n",
    "- This scaper is designed to request up to 250 search per minutes, the upper limit of OneMap Rest API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool as ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiThread Process for collection of Geo Data via OneMap API\n",
    "\n",
    "def geoget(url):\n",
    "    try:\n",
    "        result_temp = requests.get(url).json()['results'][0]\n",
    "        sv = result_temp['SEARCHVAL']\n",
    "        blk = result_temp['BLK_NO']\n",
    "        road = result_temp['ROAD_NAME']\n",
    "        build = result_temp['BUILDING']\n",
    "        add = result_temp['ADDRESS']\n",
    "        post = result_temp['POSTAL']\n",
    "        x = result_temp['X']\n",
    "        y = result_temp['Y']\n",
    "        lat = result_temp['LATITUDE']\n",
    "        lon = result_temp['LONGITUDE']\n",
    "        long = result_temp['LONGTITUDE']\n",
    "    except:\n",
    "        sv = ''\n",
    "        blk = ''\n",
    "        road = ''\n",
    "        build = ''\n",
    "        add = ''\n",
    "        post = ''\n",
    "        x = ''\n",
    "        y = ''\n",
    "        lat = ''\n",
    "        lon = ''\n",
    "        long = ''\n",
    "    return sv, blk, road, build, add, post, x, y, lat, lon, long\n",
    "\n",
    "def getDir_index(dirPath):\n",
    "    listOfFile = os.listdir(dirPath)\n",
    "    dir_index = []\n",
    "    for each in listOfFile:\n",
    "        dir_index.append(int(each.split('.')[0].split('_')[1]))\n",
    "    return dir_index\n",
    "\n",
    "def getLinks(df,column1,column2=None,column3=None):\n",
    "    \n",
    "    links=[]\n",
    "    if column3 != None:\n",
    "        for each in zip(df[column1], df[column2], df[column3]):\n",
    "            search_value = (str(each[0])+' '+str(each[1]+' '+str(each[2])))\n",
    "            apiurl = 'https://developers.onemap.sg/commonapi/search?searchVal='+search_value+'&returnGeom=Y&getAddrDetails=Y&pageNum=1'\n",
    "            links.append(apiurl)\n",
    "    elif column2 != None:\n",
    "        for each in zip(df[column1], df[column2]):\n",
    "            search_value = (str(each[0])+\" \"+str(each[1]))\n",
    "            apiurl = 'https://developers.onemap.sg/commonapi/search?searchVal='+search_value+'&returnGeom=Y&getAddrDetails=Y&pageNum=1'\n",
    "            links.append(apiurl)\n",
    "    else:\n",
    "        for each in df[column1]:\n",
    "            search_value = (str(each))\n",
    "            apiurl = 'https://developers.onemap.sg/commonapi/search?searchVal='+search_value+'&returnGeom=Y&getAddrDetails=Y&pageNum=1'\n",
    "            links.append(apiurl)\n",
    "        \n",
    "    return links\n",
    "\n",
    "\n",
    "def generator(df, links, dirPath, filename):\n",
    "    \n",
    "    lowerlimit = 0\n",
    "    upperlimit = 250\n",
    "    dir_index = getDir_index(dirPath) \n",
    "    \n",
    "    for i in range(1,math.ceil(df.shape[0]/250)+1):\n",
    "        start = time.perf_counter()\n",
    "        if i in dir_index:\n",
    "            lowerlimit += 250\n",
    "            upperlimit += 250\n",
    "            continue\n",
    "        else:\n",
    "            start_processing = time.perf_counter()\n",
    "            \n",
    "            limited_links = links[lowerlimit:upperlimit]\n",
    "            limited_df = df[lowerlimit:upperlimit].copy()\n",
    "            limited_df.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            pool = ThreadPool(4)  # Make the Pool of workers\n",
    "            results = pool.map(geoget, limited_links) #Open the urls in their own threads\n",
    "            pool.close() #close the pool and wait for the work to finish \n",
    "            pool.join()\n",
    "            \n",
    "            sv = []\n",
    "            blk = []\n",
    "            road = []\n",
    "            build = []\n",
    "            add = []\n",
    "            post = []\n",
    "            x = []\n",
    "            y = []\n",
    "            lat = []\n",
    "            lon = []\n",
    "            long = []\n",
    "\n",
    "            for each in results:\n",
    "                sv.append(each[0])\n",
    "                blk.append(each[1])\n",
    "                road.append(each[2])\n",
    "                build.append(each[3])\n",
    "                add.append(each[4])\n",
    "                post.append(each[5])\n",
    "                x.append(each[6])\n",
    "                y.append(each[7])\n",
    "                lat.append(each[8])\n",
    "                lon.append(each[9])\n",
    "                long.append(each[10])\n",
    "\n",
    "            data = pd.DataFrame(list(zip(sv,blk,road,build,add,post,x,y,lat,lon,long)),\n",
    "                                columns =['SEARCHVAL','BLK_NO','ROAD_NAME','BUILDING','ADDRESS','POSTAL','X','Y','LATITUDE','LONGITUDE','LONGTITUDE'])\n",
    "\n",
    "            limited_df = pd.concat([limited_df, data], axis=1)\n",
    "            limited_df.to_csv(dirPath+'\\\\'+filename+'_'+\"{:04n}\".format(i)+'.csv',index=False)\n",
    "\n",
    "            finish_processing = time.perf_counter()\n",
    "\n",
    "            if finish_processing-start_processing > 60.5:\n",
    "                time.sleep(0)\n",
    "            else:\n",
    "                time.sleep(round(60.5-(finish_processing-start_processing),0))\n",
    "\n",
    "            finish = time.perf_counter()\n",
    "            print(f'Finished data range {i} for rows {lowerlimit} to {upperlimit} in {round(finish-start,2)} second(s)')\n",
    "\n",
    "            lowerlimit += 250\n",
    "            upperlimit += 250\n",
    "            \n",
    "def main(df,dirPath,filename,column1,column2=None,column3=None):\n",
    "    \n",
    "    if isinstance(df, pd.DataFrame)==False:\n",
    "        print('Input is not DataFrame')\n",
    "        return\n",
    "    \n",
    "    if column1 == column2 or column1 == column3 or column2 == column3 and column3 != None:\n",
    "        print('Unique column is needed, cannot use duplicated column')\n",
    "        return\n",
    "    \n",
    "    if os.path.isdir(dirPath) == False:\n",
    "        if os.path.splitext(dirPath)[1] == '':\n",
    "            os.makedirs(dirPath)\n",
    "        else:\n",
    "            print('Given directory is a file and not a folder, please key in new directory')\n",
    "            return\n",
    "    \n",
    "    links = getLinks(df,column1,column2,column3)\n",
    "    \n",
    "    incomplete = True\n",
    "    while incomplete == True:\n",
    "        \n",
    "        generator(df,links,dirPath,filename)\n",
    "        \n",
    "        dir_index = getDir_index(dirPath)\n",
    "        cur_count = len(dir_index)\n",
    "        tar_count = math.ceil(df.shape[0]/250)\n",
    "        if cur_count == tar_count:\n",
    "            incomplete = False\n",
    "            print(f'All {tar_count} geoData file segments had being scraped successfully')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "def getListOfFiles(dirName):\n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    for entry in listOfFile:\n",
    "        fullPath = os.path.join(dirName,entry)\n",
    "        allFiles.append(fullPath)\n",
    "    return allFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "userhome = os.path.join(Path.home(),'Documents')\n",
    "\n",
    "# The sample data used would be from https://data.gov.sg/dataset/resale-flat-prices \n",
    "\n",
    "# Import data into pandas dataframe\n",
    "df_flats = pd.read_csv('resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv')\n",
    "df_flats = df_flats[0:1000].copy()\n",
    "\n",
    "# Running main() to REST API GeoData from OneMap.gov.sg\n",
    "if __name__ == \"__main__\":\n",
    "    df=df_flats\n",
    "    dirPath=os.path.join(userhome,'OneMap_API_GeoData')\n",
    "    filename='GeoData'\n",
    "    column1='block'\n",
    "    column2='street_name'\n",
    "    column3=None\n",
    "    main(df,dirPath,filename,column1,column2,column3)\n",
    "    \n",
    "    # Loading up the collected GeoData and re-merge them into a single dataframe\n",
    "    list_geo=[]\n",
    "    list_of_geo_files = getListOfFiles(dirPath)\n",
    "    for filename in list_of_geo_files:\n",
    "        df = pd.read_csv(filename,index_col=None,header=0)\n",
    "        list_geo.append(df)\n",
    "    df_flats = pd.concat(list_geo,axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
